import requests
import json

neptune_endpoint = "https://db-neptune-2.cluster-c8qttgkgfep5.us-east-1.neptune.amazonaws.com:8182/gremlin"
headers = {"Content-Type": "application/json"}

def send_gremlin(query):
    payload = {"gremlin": query}
    try:
        response = requests.post(neptune_endpoint, headers=headers, data=json.dumps(payload))
        if response.status_code != 200:
            print(f"‚ùå Query Failed:\n{query}\nResponse:\n{response.text}")
        return response.status_code == 200
    except Exception as e:
        print(f"‚ùå Exception: {e}")
        return False
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22e4c4d-f5d1-4243-b041-6602344d6ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_gremlin(\"g.V().drop()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ea1f58-b3ec-4d85-abd7-7f5c21b351c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Clearing existing Neptune graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base Companies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 24.93it/s]\n",
      "Associated Companies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.54it/s]\n",
      "Linkage Types: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.31it/s]\n",
      "Linkage Values: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 27.68it/s]\n",
      "Edges: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 42.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Expanding Base ‚ûù Type ‚ûù Value ‚ûù Assoc in Explorer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-expanding Graph Explorer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ COMPLETE: Neptune graph is fully interactive and properly hierarchical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==== Neptune Config ====\n",
    "neptune_endpoint = \"https://db-neptune-2.cluster-c8qttgkgfep5.us-east-1.neptune.amazonaws.com:8182/gremlin\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "threshold = 3000  # Only base companies above this threshold will be included\n",
    "\n",
    "def send_gremlin(query):\n",
    "    payload = {\"gremlin\": query}\n",
    "    try:\n",
    "        response = requests.post(neptune_endpoint, headers=headers, data=json.dumps(payload))\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå {query}\\n{response.text}\")\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {e}\")\n",
    "        return False\n",
    "\n",
    "# ==== Load and Filter Dataset ====\n",
    "df = pd.read_csv(\"Updated_Top_Companies_with_Evidence_Clean(in).csv\")\n",
    "df_filtered = df[df['Company_Evidence_Weight'] >= threshold].dropna(subset=[\"Company_Evidence_Count\"])\n",
    "\n",
    "# ==== Reset Graph ====\n",
    "print(\"üßπ Clearing existing Neptune graph...\")\n",
    "send_gremlin(\"g.V().drop()\")\n",
    "\n",
    "# ==== Tracking Sets ====\n",
    "base_companies = {}\n",
    "associated_companies = {}\n",
    "linkage_types = set()\n",
    "linkage_values = {}\n",
    "edges = set()\n",
    "\n",
    "# ==== Build Nodes and Relationships ====\n",
    "for _, row in df_filtered.iterrows():\n",
    "    base = row['Base_Consolidation_Name'].strip().replace(\"'\", \"\")\n",
    "    assoc = row['Associated_Company_Name'].strip().replace(\"'\", \"\")\n",
    "    method = row['Linkage_Method'].strip().replace(\"'\", \"\")\n",
    "    value = row['Linkage_Value'].strip().replace(\"'\", \"\")\n",
    "    weight = int(row['Company_Evidence_Weight'])\n",
    "    count = int(row['Company_Evidence_Count'])\n",
    "    assoc_count = int(row['Association_companies_Count'])\n",
    "\n",
    "    lv_key = f\"{method}::{value}\"\n",
    "\n",
    "    # Register unique nodes\n",
    "    base_companies[base] = (base, weight, count, assoc_count)\n",
    "    associated_companies[assoc] = (assoc, weight, count, assoc_count)\n",
    "    linkage_types.add(method)\n",
    "    linkage_values[lv_key] = (method, value)\n",
    "\n",
    "    # Only 1 edge from base ‚ûù type\n",
    "    edges.add((base, 'HAS_TYPE', method))\n",
    "    edges.add((method, 'HAS_VALUE', lv_key))\n",
    "    edges.add((lv_key, 'SHARED_WITH', assoc))\n",
    "\n",
    "# ==== Upload Base Company Nodes ====\n",
    "for name, (n, w, c, a) in tqdm(base_companies.items(), desc=\"Base Companies\"):\n",
    "    q = f\"g.V().has('name', '{n}').fold().coalesce(unfold(), addV('BaseCompany')\" \\\n",
    "        f\".property('name', '{n}').property('evidence_weight', {w})\" \\\n",
    "        f\".property('evidence_count', {c}).property('assoc_count', {a}))\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Upload Associated Company Nodes ====\n",
    "for name, (n, w, c, a) in tqdm(associated_companies.items(), desc=\"Associated Companies\"):\n",
    "    q = f\"g.V().has('name', '{n}').fold().coalesce(unfold(), addV('AssociatedCompany')\" \\\n",
    "        f\".property('name', '{n}').property('evidence_weight', {w})\" \\\n",
    "        f\".property('evidence_count', {c}).property('assoc_count', {a}))\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Upload Linkage Type Nodes ====\n",
    "for lt in tqdm(linkage_types, desc=\"Linkage Types\"):\n",
    "    q = f\"g.V().has('name', '{lt}').fold().coalesce(unfold(), addV('LinkageType')\" \\\n",
    "        f\".property('name', '{lt}'))\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Upload Linkage Value Nodes ====\n",
    "for lv_key, (method, val) in tqdm(linkage_values.items(), desc=\"Linkage Values\"):\n",
    "    q = f\"g.V().has('name', '{lv_key}').fold().coalesce(unfold(), addV('LinkageValue')\" \\\n",
    "        f\".property('name', '{lv_key}').property('method', '{method}').property('value', '{val}'))\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Upload All Edges (no duplicates) ====\n",
    "for src, label, dst in tqdm(edges, desc=\"Edges\"):\n",
    "    q = f\"g.V().has('name', '{src}').as('a').V().has('name', '{dst}').addE('{label}').from('a')\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Expand Full Paths ====\n",
    "print(\"üì° Expanding Base ‚ûù Type ‚ûù Value ‚ûù Assoc in Explorer...\")\n",
    "for base in tqdm(base_companies, desc=\"Auto-expanding Graph Explorer\"):\n",
    "    q = f\"g.V().hasLabel('BaseCompany').has('name', '{base}')\" \\\n",
    "        f\".repeat(out().simplePath()).times(3).path()\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "print(\"\\n‚úÖ COMPLETE: Neptune graph is fully interactive and properly hierarchical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a520e26-13f2-4d44-ae87-8bb260f122b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Dropping old graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base Companies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  9.34it/s]\n",
      "Associated Companies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 183/183 [00:10<00:00, 18.12it/s]\n",
      "Linkage Types: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 27.94it/s]\n",
      "Linkage Values: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:02<00:00, 27.04it/s]\n",
      "Edges: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [00:20<00:00, 28.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Expanding Base ‚ûù Type ‚ûù Value ‚ûù Assoc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph Explorer Path Expansion: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ COMPLETE: Graph with threshold ‚â• 200 is now live in Explorer with full expansion!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==== Neptune Config ====\n",
    "neptune_endpoint = \"https://db-neptune-2.cluster-c8qttgkgfep5.us-east-1.neptune.amazonaws.com:8182/gremlin\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "threshold = 2000  # ‚Üê Updated Threshold for Base Companies\n",
    "\n",
    "def send_gremlin(query):\n",
    "    payload = {\"gremlin\": query}\n",
    "    try:\n",
    "        response = requests.post(neptune_endpoint, headers=headers, data=json.dumps(payload))\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå {query}\\n{response.text}\")\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {e}\")\n",
    "        return False\n",
    "\n",
    "# ==== Load and Filter Dataset ====\n",
    "df = pd.read_csv(\"Updated_Top_Companies_with_Evidence_Clean(in).csv\")\n",
    "df_filtered = df[df['Company_Evidence_Weight'] >= threshold].dropna(subset=[\"Company_Evidence_Count\"])\n",
    "\n",
    "# ==== Clear Existing Graph ====\n",
    "print(\"üßπ Dropping old graph...\")\n",
    "send_gremlin(\"g.V().drop()\")\n",
    "\n",
    "# ==== Track All Nodes and Edges ====\n",
    "base_companies = {}\n",
    "associated_companies = {}\n",
    "linkage_types = set()\n",
    "linkage_values = {}\n",
    "edges = set()\n",
    "\n",
    "# ==== Build Graph ====\n",
    "for _, row in df_filtered.iterrows():\n",
    "    base = row['Base_Consolidation_Name'].strip().replace(\"'\", \"\")\n",
    "    assoc = row['Associated_Company_Name'].strip().replace(\"'\", \"\")\n",
    "    method = row['Linkage_Method'].strip().replace(\"'\", \"\")\n",
    "    value = row['Linkage_Value'].strip().replace(\"'\", \"\")\n",
    "    weight = int(row['Company_Evidence_Weight'])\n",
    "    count = int(row['Company_Evidence_Count'])\n",
    "    assoc_count = int(row['Association_companies_Count'])\n",
    "\n",
    "    lv_key = f\"{method}::{value}\"\n",
    "\n",
    "    base_companies[base] = (base, weight, count, assoc_count)\n",
    "    associated_companies[assoc] = (assoc, weight, count, assoc_count)\n",
    "    linkage_types.add(method)\n",
    "    linkage_values[lv_key] = (method, value)\n",
    "\n",
    "    # Edges\n",
    "    edges.add((base, 'HAS_TYPE', method))\n",
    "    edges.add((method, 'HAS_VALUE', lv_key))\n",
    "    edges.add((lv_key, 'SHARED_WITH', assoc))\n",
    "\n",
    "# ==== Insert Base Companies ====\n",
    "for name, (n, w, c, a) in tqdm(base_companies.items(), desc=\"Base Companies\"):\n",
    "    q = f\"g.V().has('name', '{n}').fold().coalesce(unfold(), addV('BaseCompany')\" \\\n",
    "        f\".property('name', '{n}').property('evidence_weight', {w})\" \\\n",
    "        f\".property('evidence_count', {c}).property('assoc_count', {a}))\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Insert Associated Companies ====\n",
    "for name, (n, w, c, a) in tqdm(associated_companies.items(), desc=\"Associated Companies\"):\n",
    "    q = f\"g.V().has('name', '{n}').fold().coalesce(unfold(), addV('AssociatedCompany')\" \\\n",
    "        f\".property('name', '{n}').property('evidence_weight', {w})\" \\\n",
    "        f\".property('evidence_count', {c}).property('assoc_count', {a}))\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Insert Linkage Types ====\n",
    "for lt in tqdm(linkage_types, desc=\"Linkage Types\"):\n",
    "    q = f\"g.V().has('name', '{lt}').fold().coalesce(unfold(), addV('LinkageType')\" \\\n",
    "        f\".property('name', '{lt}'))\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Insert Linkage Values ====\n",
    "for lv_key, (method, val) in tqdm(linkage_values.items(), desc=\"Linkage Values\"):\n",
    "    q = f\"g.V().has('name', '{lv_key}').fold().coalesce(unfold(), addV('LinkageValue')\" \\\n",
    "        f\".property('name', '{lv_key}').property('method', '{method}').property('value', '{val}'))\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Insert All Edges (deduped) ====\n",
    "for src, label, dst in tqdm(edges, desc=\"Edges\"):\n",
    "    q = f\"g.V().has('name', '{src}').as('a').V().has('name', '{dst}').addE('{label}').from('a')\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "# ==== Push to Graph Explorer ====\n",
    "print(\"üì° Expanding Base ‚ûù Type ‚ûù Value ‚ûù Assoc...\")\n",
    "for base in tqdm(base_companies, desc=\"Graph Explorer Path Expansion\"):\n",
    "    q = f\"g.V().hasLabel('BaseCompany').has('name', '{base}')\" \\\n",
    "        f\".repeat(out().simplePath()).times(3).path()\"\n",
    "    send_gremlin(q)\n",
    "\n",
    "print(\"\\n‚úÖ COMPLETE: Graph with threshold ‚â• 200 is now live in Explorer with full expansion!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973a1f7-ef48-44ba-8049-d5666ba8d23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base Companies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 19.33it/s]\n",
      "Associated Companies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 638/638 [00:14<00:00, 43.99it/s]\n",
      "Linkage Types: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 39.14it/s]\n",
      "Linkage Values: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 444/444 [00:09<00:00, 46.99it/s]\n",
      "Edges: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3005/3005 [06:21<00:00,  7.87it/s]\n",
      "Expand From Tier:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 500: {\"detailedMessage\":\"Query cannot be completed due to memory limitations.\",\"requestId\":\"d7530591-a945-4a00-941b-a8aaa7fde3af\",\"code\":\"MemoryLimitExceededException\",\"message\":\"Query cannot be completed due to memory limitations.\"}\n",
      "Error 500: {\"detailedMessage\":\"Query cannot be completed due to memory limitations.\",\"requestId\":\"72e29ec0-9bc9-4cc0-a187-4ee3a28bf694\",\"code\":\"MemoryLimitExceededException\",\"message\":\"Query cannot be completed due to memory limitations.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expand From Tier:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 500: {\"detailedMessage\":\"Query cannot be completed due to memory limitations.\",\"requestId\":\"ea4f206e-ae87-4544-be0e-28a0a71a2515\",\"code\":\"MemoryLimitExceededException\",\"message\":\"Query cannot be completed due to memory limitations.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expand From Tier: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Neptune Config ===\n",
    "neptune_endpoint = \"https://db-neptune-2.cluster-c8qttgkgfep5.us-east-1.neptune.amazonaws.com:8182/gremlin\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# === Dataset Load ===\n",
    "df = pd.read_csv(\"Updated_Top_Companies_with_Evidence_Clean(in).csv\")\n",
    "df = df.dropna(subset=[\"Company_Evidence_Count\"])\n",
    "\n",
    "# === Create Evidence Tiers ===\n",
    "df['Weight_Tier'] = pd.cut(\n",
    "    df['Company_Evidence_Weight'],\n",
    "    bins=[0, 1999, 2999, float('inf')],\n",
    "    labels=[\"< 2000\", \"2000 - 2999\", \"3000+\"],\n",
    "    right=True\n",
    ")\n",
    "\n",
    "# === Gremlin Sender ===\n",
    "def send_gremlin(query, delay=0.1, retries=3):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            payload = {\"gremlin\": query}\n",
    "            res = requests.post(neptune_endpoint, headers=headers, data=json.dumps(payload), timeout=10)\n",
    "            if res.status_code == 200:\n",
    "                return True\n",
    "            print(f\"Error {res.status_code}: {res.text}\")\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\", e)\n",
    "        time.sleep(delay)\n",
    "    return False\n",
    "\n",
    "# === Drop Graph ===\n",
    "send_gremlin(\"g.V().drop()\")\n",
    "\n",
    "# === Track Nodes ===\n",
    "tiers, base_nodes, assoc_nodes = set(), {}, {}\n",
    "linkage_types, linkage_values, edges = set(), {}, set()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tier = row['Weight_Tier']\n",
    "    tiers.add(tier)\n",
    "\n",
    "    base = row['Base_Consolidation_Name'].strip().replace(\"'\", \"\")\n",
    "    assoc = row['Associated_Company_Name'].strip().replace(\"'\", \"\")\n",
    "    method = row['Linkage_Method'].strip().replace(\"'\", \"\")\n",
    "    value = row['Linkage_Value'].strip().replace(\"'\", \"\")\n",
    "    lv_key = f\"{method}::{value}\"\n",
    "    \n",
    "    weight = int(row['Company_Evidence_Weight'])\n",
    "    count = int(row['Company_Evidence_Count'])\n",
    "    assoc_count = int(row['Association_companies_Count'])\n",
    "\n",
    "    base_nodes[(tier, base)] = (base, weight, count, assoc_count)\n",
    "    assoc_nodes[(tier, assoc)] = (assoc, weight, count, assoc_count)\n",
    "    linkage_types.add((tier, method))\n",
    "    linkage_values[(tier, lv_key)] = (method, value)\n",
    "\n",
    "    edges.add((tier, 'HAS_BASE', base))\n",
    "    edges.add((base, 'HAS_TYPE', method))\n",
    "    edges.add((method, 'HAS_VALUE', lv_key))\n",
    "    edges.add((lv_key, 'SHARED_WITH', assoc))\n",
    "\n",
    "# === Insert Nodes ===\n",
    "for tier in tiers:\n",
    "    send_gremlin(f\"g.addV('EvidenceTier').property('name', 'Evidence: {tier}')\")\n",
    "\n",
    "for (tier, name), (n, w, c, a) in tqdm(base_nodes.items(), desc=\"Base Companies\"):\n",
    "    send_gremlin(f\"g.addV('BaseCompany').property('name', '{n}')\" +\n",
    "                 f\".property('evidence_weight', {w}).property('evidence_count', {c}).property('assoc_count', {a})\")\n",
    "    send_gremlin(f\"g.V().has('name', 'Evidence: {tier}').as('t')\" +\n",
    "                 f\".V().has('name', '{n}').addE('HAS_BASE').from('t')\")\n",
    "\n",
    "for (tier, name), (n, w, c, a) in tqdm(assoc_nodes.items(), desc=\"Associated Companies\"):\n",
    "    send_gremlin(f\"g.addV('AssociatedCompany').property('name', '{n}')\" +\n",
    "                 f\".property('evidence_weight', {w}).property('evidence_count', {c}).property('assoc_count', {a})\")\n",
    "\n",
    "for (tier, method) in tqdm(linkage_types, desc=\"Linkage Types\"):\n",
    "    send_gremlin(f\"g.addV('LinkageType').property('name', '{method}')\")\n",
    "\n",
    "for (tier, lv_key), (method, val) in tqdm(linkage_values.items(), desc=\"Linkage Values\"):\n",
    "    send_gremlin(f\"g.addV('LinkageValue').property('name', '{lv_key}')\" +\n",
    "                 f\".property('method', '{method}').property('value', '{val}')\")\n",
    "\n",
    "for src, label, dst in tqdm(edges, desc=\"Edges\"):\n",
    "    send_gremlin(f\"g.V().has('name', '{src}').as('a').V().has('name', '{dst}').addE('{label}').from('a')\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# === Auto Expand from Tier ‚ûù Base ‚ûù Type ‚ûù Value ‚ûù Assoc\n",
    "for tier in tqdm(tiers, desc=\"Expand From Tier\"):\n",
    "    send_gremlin(f\"g.V().hasLabel('EvidenceTier').has('name', 'Evidence: {tier}')\" +\n",
    "                 \".out('HAS_BASE').out().out().out().path()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1d3e1-8837-47c4-bc7b-8846cd380d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
